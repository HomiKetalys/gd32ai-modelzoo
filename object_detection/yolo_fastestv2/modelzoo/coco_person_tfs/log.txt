训练配置:
{'model_name': 'coco_person', 'epochs': 300, 'steps': [150.0, 250.0], 'batch_size': 128, 'subdivisions': 1, 'learning_rate': 0.001, 'pre_weights': 'None', 'classes': 1, 'width': 256, 'height': 256, 'anchor_num': 3, 'separation': 3, 'separation_scale': 2, 'anchors': [6.21, 14.8, 18.67, 43.63, 38.13, 100.64, 75.28, 171.12, 143.62, 75.3, 173.09, 209.01], 'conf_thr': 0.001, 'nms_thr': 0.5, 'iou_thr': 0.4, 'label_flag': 'coco_person', 'val': '../../../datasets/coco2017/val2017_person.txt', 'train': '../../../datasets/coco2017/train2017_person.txt', 'names': './configs/coco_person.names'}
load param...
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 24, 64, 64]             648
       BatchNorm2d-2           [-1, 24, 64, 64]              48
              ReLU-3           [-1, 24, 64, 64]               0
         MaxPool2d-4           [-1, 24, 32, 32]               0
            Conv2d-5           [-1, 24, 16, 16]             216
       BatchNorm2d-6           [-1, 24, 16, 16]              48
            Conv2d-7           [-1, 24, 16, 16]             576
       BatchNorm2d-8           [-1, 24, 16, 16]              48
              ReLU-9           [-1, 24, 16, 16]               0
           Conv2d-10           [-1, 24, 32, 32]             576
      BatchNorm2d-11           [-1, 24, 32, 32]              48
             ReLU-12           [-1, 24, 32, 32]               0
           Conv2d-13           [-1, 24, 16, 16]             216
      BatchNorm2d-14           [-1, 24, 16, 16]              48
           Conv2d-15           [-1, 24, 16, 16]             576
      BatchNorm2d-16           [-1, 24, 16, 16]              48
             ReLU-17           [-1, 24, 16, 16]               0
  ShuffleV2Block_-18           [-1, 48, 16, 16]               0
           Conv2d-19           [-1, 24, 16, 16]             576
      BatchNorm2d-20           [-1, 24, 16, 16]              48
             ReLU-21           [-1, 24, 16, 16]               0
           Conv2d-22           [-1, 24, 16, 16]             216
      BatchNorm2d-23           [-1, 24, 16, 16]              48
           Conv2d-24           [-1, 24, 16, 16]             576
      BatchNorm2d-25           [-1, 24, 16, 16]              48
             ReLU-26           [-1, 24, 16, 16]               0
  ShuffleV2Block_-27           [-1, 48, 16, 16]               0
           Conv2d-28           [-1, 24, 16, 16]             576
      BatchNorm2d-29           [-1, 24, 16, 16]              48
             ReLU-30           [-1, 24, 16, 16]               0
           Conv2d-31           [-1, 24, 16, 16]             216
      BatchNorm2d-32           [-1, 24, 16, 16]              48
           Conv2d-33           [-1, 24, 16, 16]             576
      BatchNorm2d-34           [-1, 24, 16, 16]              48
             ReLU-35           [-1, 24, 16, 16]               0
  ShuffleV2Block_-36           [-1, 48, 16, 16]               0
           Conv2d-37           [-1, 24, 16, 16]             576
      BatchNorm2d-38           [-1, 24, 16, 16]              48
             ReLU-39           [-1, 24, 16, 16]               0
           Conv2d-40           [-1, 24, 16, 16]             216
      BatchNorm2d-41           [-1, 24, 16, 16]              48
           Conv2d-42           [-1, 24, 16, 16]             576
      BatchNorm2d-43           [-1, 24, 16, 16]              48
             ReLU-44           [-1, 24, 16, 16]               0
  ShuffleV2Block_-45           [-1, 48, 16, 16]               0
           Conv2d-46           [-1, 48, 16, 16]             432
      BatchNorm2d-47           [-1, 48, 16, 16]              96
           Conv2d-48           [-1, 48, 16, 16]           2,304
      BatchNorm2d-49           [-1, 48, 16, 16]              96
             ReLU-50           [-1, 48, 16, 16]               0
           Conv2d-51           [-1, 48, 32, 32]           2,304
      BatchNorm2d-52           [-1, 48, 32, 32]              96
             ReLU-53           [-1, 48, 32, 32]               0
           Conv2d-54           [-1, 48, 16, 16]             432
      BatchNorm2d-55           [-1, 48, 16, 16]              96
           Conv2d-56           [-1, 48, 16, 16]           2,304
      BatchNorm2d-57           [-1, 48, 16, 16]              96
             ReLU-58           [-1, 48, 16, 16]               0
  ShuffleV2Block_-59           [-1, 96, 16, 16]               0
           Conv2d-60           [-1, 48, 16, 16]           2,304
      BatchNorm2d-61           [-1, 48, 16, 16]              96
             ReLU-62           [-1, 48, 16, 16]               0
           Conv2d-63           [-1, 48, 16, 16]             432
      BatchNorm2d-64           [-1, 48, 16, 16]              96
           Conv2d-65           [-1, 48, 16, 16]           2,304
      BatchNorm2d-66           [-1, 48, 16, 16]              96
             ReLU-67           [-1, 48, 16, 16]               0
  ShuffleV2Block_-68           [-1, 96, 16, 16]               0
           Conv2d-69           [-1, 48, 16, 16]           2,304
      BatchNorm2d-70           [-1, 48, 16, 16]              96
             ReLU-71           [-1, 48, 16, 16]               0
           Conv2d-72           [-1, 48, 16, 16]             432
      BatchNorm2d-73           [-1, 48, 16, 16]              96
           Conv2d-74           [-1, 48, 16, 16]           2,304
      BatchNorm2d-75           [-1, 48, 16, 16]              96
             ReLU-76           [-1, 48, 16, 16]               0
  ShuffleV2Block_-77           [-1, 96, 16, 16]               0
           Conv2d-78           [-1, 48, 16, 16]           2,304
      BatchNorm2d-79           [-1, 48, 16, 16]              96
             ReLU-80           [-1, 48, 16, 16]               0
           Conv2d-81           [-1, 48, 16, 16]             432
      BatchNorm2d-82           [-1, 48, 16, 16]              96
           Conv2d-83           [-1, 48, 16, 16]           2,304
      BatchNorm2d-84           [-1, 48, 16, 16]              96
             ReLU-85           [-1, 48, 16, 16]               0
  ShuffleV2Block_-86           [-1, 96, 16, 16]               0
           Conv2d-87           [-1, 48, 16, 16]           2,304
      BatchNorm2d-88           [-1, 48, 16, 16]              96
             ReLU-89           [-1, 48, 16, 16]               0
           Conv2d-90           [-1, 48, 16, 16]             432
      BatchNorm2d-91           [-1, 48, 16, 16]              96
           Conv2d-92           [-1, 48, 16, 16]           2,304
      BatchNorm2d-93           [-1, 48, 16, 16]              96
             ReLU-94           [-1, 48, 16, 16]               0
  ShuffleV2Block_-95           [-1, 96, 16, 16]               0
           Conv2d-96           [-1, 48, 16, 16]           2,304
      BatchNorm2d-97           [-1, 48, 16, 16]              96
             ReLU-98           [-1, 48, 16, 16]               0
           Conv2d-99           [-1, 48, 16, 16]             432
     BatchNorm2d-100           [-1, 48, 16, 16]              96
          Conv2d-101           [-1, 48, 16, 16]           2,304
     BatchNorm2d-102           [-1, 48, 16, 16]              96
            ReLU-103           [-1, 48, 16, 16]               0
 ShuffleV2Block_-104           [-1, 96, 16, 16]               0
          Conv2d-105           [-1, 48, 16, 16]           2,304
     BatchNorm2d-106           [-1, 48, 16, 16]              96
            ReLU-107           [-1, 48, 16, 16]               0
          Conv2d-108           [-1, 48, 16, 16]             432
     BatchNorm2d-109           [-1, 48, 16, 16]              96
          Conv2d-110           [-1, 48, 16, 16]           2,304
     BatchNorm2d-111           [-1, 48, 16, 16]              96
            ReLU-112           [-1, 48, 16, 16]               0
 ShuffleV2Block_-113           [-1, 96, 16, 16]               0
          Conv2d-114           [-1, 48, 16, 16]           2,304
     BatchNorm2d-115           [-1, 48, 16, 16]              96
            ReLU-116           [-1, 48, 16, 16]               0
          Conv2d-117           [-1, 48, 16, 16]             432
     BatchNorm2d-118           [-1, 48, 16, 16]              96
          Conv2d-119           [-1, 48, 16, 16]           2,304
     BatchNorm2d-120           [-1, 48, 16, 16]              96
            ReLU-121           [-1, 48, 16, 16]               0
 ShuffleV2Block_-122           [-1, 96, 16, 16]               0
          Conv2d-123             [-1, 96, 8, 8]             864
     BatchNorm2d-124             [-1, 96, 8, 8]             192
          Conv2d-125             [-1, 96, 8, 8]           9,216
     BatchNorm2d-126             [-1, 96, 8, 8]             192
            ReLU-127             [-1, 96, 8, 8]               0
          Conv2d-128           [-1, 96, 16, 16]           9,216
     BatchNorm2d-129           [-1, 96, 16, 16]             192
            ReLU-130           [-1, 96, 16, 16]               0
          Conv2d-131             [-1, 96, 8, 8]             864
     BatchNorm2d-132             [-1, 96, 8, 8]             192
          Conv2d-133             [-1, 96, 8, 8]           9,216
     BatchNorm2d-134             [-1, 96, 8, 8]             192
            ReLU-135             [-1, 96, 8, 8]               0
 ShuffleV2Block_-136            [-1, 192, 8, 8]               0
          Conv2d-137             [-1, 96, 8, 8]           9,216
     BatchNorm2d-138             [-1, 96, 8, 8]             192
            ReLU-139             [-1, 96, 8, 8]               0
          Conv2d-140             [-1, 96, 8, 8]             864
     BatchNorm2d-141             [-1, 96, 8, 8]             192
          Conv2d-142             [-1, 96, 8, 8]           9,216
     BatchNorm2d-143             [-1, 96, 8, 8]             192
            ReLU-144             [-1, 96, 8, 8]               0
 ShuffleV2Block_-145            [-1, 192, 8, 8]               0
          Conv2d-146             [-1, 96, 8, 8]           9,216
     BatchNorm2d-147             [-1, 96, 8, 8]             192
            ReLU-148             [-1, 96, 8, 8]               0
          Conv2d-149             [-1, 96, 8, 8]             864
     BatchNorm2d-150             [-1, 96, 8, 8]             192
          Conv2d-151             [-1, 96, 8, 8]           9,216
     BatchNorm2d-152             [-1, 96, 8, 8]             192
            ReLU-153             [-1, 96, 8, 8]               0
 ShuffleV2Block_-154            [-1, 192, 8, 8]               0
          Conv2d-155             [-1, 96, 8, 8]           9,216
     BatchNorm2d-156             [-1, 96, 8, 8]             192
            ReLU-157             [-1, 96, 8, 8]               0
          Conv2d-158             [-1, 96, 8, 8]             864
     BatchNorm2d-159             [-1, 96, 8, 8]             192
          Conv2d-160             [-1, 96, 8, 8]           9,216
     BatchNorm2d-161             [-1, 96, 8, 8]             192
            ReLU-162             [-1, 96, 8, 8]               0
 ShuffleV2Block_-163            [-1, 192, 8, 8]               0
  ShuffleNetV2Sp-164  [[-1, 96, 16, 16], [-1, 192, 8, 8]]               0
          Conv2d-165             [-1, 72, 8, 8]          13,824
     BatchNorm2d-166             [-1, 72, 8, 8]             144
            ReLU-167             [-1, 72, 8, 8]               0
          Conv2d-168             [-1, 72, 8, 8]           1,800
     BatchNorm2d-169             [-1, 72, 8, 8]             144
            ReLU-170             [-1, 72, 8, 8]               0
          Conv2d-171             [-1, 72, 8, 8]           5,184
     BatchNorm2d-172             [-1, 72, 8, 8]             144
          Conv2d-173             [-1, 72, 8, 8]           1,800
     BatchNorm2d-174             [-1, 72, 8, 8]             144
            ReLU-175             [-1, 72, 8, 8]               0
          Conv2d-176             [-1, 72, 8, 8]           5,184
     BatchNorm2d-177             [-1, 72, 8, 8]             144
     DWConvblock-178             [-1, 72, 8, 8]               0
          Conv2d-179             [-1, 72, 8, 8]           1,800
     BatchNorm2d-180             [-1, 72, 8, 8]             144
            ReLU-181             [-1, 72, 8, 8]               0
          Conv2d-182             [-1, 72, 8, 8]           5,184
     BatchNorm2d-183             [-1, 72, 8, 8]             144
          Conv2d-184             [-1, 72, 8, 8]           1,800
     BatchNorm2d-185             [-1, 72, 8, 8]             144
            ReLU-186             [-1, 72, 8, 8]               0
          Conv2d-187             [-1, 72, 8, 8]           5,184
     BatchNorm2d-188             [-1, 72, 8, 8]             144
     DWConvblock-189             [-1, 72, 8, 8]               0
          Conv2d-190           [-1, 72, 16, 16]          20,736
     BatchNorm2d-191           [-1, 72, 16, 16]             144
            ReLU-192           [-1, 72, 16, 16]               0
          Conv2d-193           [-1, 72, 16, 16]           1,800
     BatchNorm2d-194           [-1, 72, 16, 16]             144
            ReLU-195           [-1, 72, 16, 16]               0
          Conv2d-196           [-1, 72, 16, 16]           5,184
     BatchNorm2d-197           [-1, 72, 16, 16]             144
          Conv2d-198           [-1, 72, 16, 16]           1,800
     BatchNorm2d-199           [-1, 72, 16, 16]             144
            ReLU-200           [-1, 72, 16, 16]               0
          Conv2d-201           [-1, 72, 16, 16]           5,184
     BatchNorm2d-202           [-1, 72, 16, 16]             144
     DWConvblock-203           [-1, 72, 16, 16]               0
          Conv2d-204           [-1, 72, 16, 16]           1,800
     BatchNorm2d-205           [-1, 72, 16, 16]             144
            ReLU-206           [-1, 72, 16, 16]               0
          Conv2d-207           [-1, 72, 16, 16]           5,184
     BatchNorm2d-208           [-1, 72, 16, 16]             144
          Conv2d-209           [-1, 72, 16, 16]           1,800
     BatchNorm2d-210           [-1, 72, 16, 16]             144
            ReLU-211           [-1, 72, 16, 16]               0
          Conv2d-212           [-1, 72, 16, 16]           5,184
     BatchNorm2d-213           [-1, 72, 16, 16]             144
     DWConvblock-214           [-1, 72, 16, 16]               0
        LightFPN-215  [[-1, 72, 16, 16], [-1, 72, 16, 16], [-1, 72, 16, 16], [-1, 72, 8, 8], [-1, 72, 8, 8], [-1, 72, 8, 8]]               0
          Conv2d-216           [-1, 12, 16, 16]             876
          Conv2d-217            [-1, 3, 16, 16]             219
          Conv2d-218            [-1, 1, 16, 16]              73
          Conv2d-219             [-1, 12, 8, 8]             876
          Conv2d-220              [-1, 3, 8, 8]             219
          Conv2d-221              [-1, 1, 8, 8]              73
================================================================
Total params: 238,496
Trainable params: 238,496
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.75
Forward/backward pass size (MB): 2283.25
Params size (MB): 0.91
Estimated Total Size (MB): 2284.91
----------------------------------------------------------------
Starting training for 300 epochs...
computer mAP...
computer PR...
epoch:0010
Precision:[0.5161494616846105]
Recall:[0.22219193020719738]
AP:[0.2517629045855733]
F1:[0.31065370688012195]
Precision_mean:0.5161494616846105
Recall_mean:0.22219193020719738
AP_mean:0.2517629045855733
F1_mean:0.31065370688012195
computer mAP...
computer PR...
epoch:0020
Precision:[0.48866913885455293]
Recall:[0.3233369683751363]
AP:[0.33133244379454085]
F1:[0.38917145200984404]
Precision_mean:0.48866913885455293
Recall_mean:0.3233369683751363
AP_mean:0.33133244379454085
F1_mean:0.38917145200984404
computer mAP...
computer PR...
epoch:0030
Precision:[0.4388711277698159]
Recall:[0.3617775354416576]
AP:[0.3361196523522571]
F1:[0.3966127023661269]
Precision_mean:0.4388711277698159
Recall_mean:0.3617775354416576
AP_mean:0.3361196523522571
F1_mean:0.3966127023661269
computer mAP...
computer PR...
epoch:0040
Precision:[0.43746870932211873]
Recall:[0.3970374409305707]
AP:[0.3762638463418493]
F1:[0.41627364108427417]
Precision_mean:0.43746870932211873
Recall_mean:0.3970374409305707
AP_mean:0.3762638463418493
F1_mean:0.41627364108427417
computer mAP...
computer PR...
epoch:0050
Precision:[0.44057623049219685]
Recall:[0.40021810250817885]
AP:[0.37223095855206634]
F1:[0.4194285714285714]
Precision_mean:0.44057623049219685
Recall_mean:0.40021810250817885
AP_mean:0.37223095855206634
F1_mean:0.4194285714285714
computer mAP...
computer PR...
epoch:0060
Precision:[0.4525540275049116]
Recall:[0.41866593965830606]
AP:[0.39618962810060837]
F1:[0.4349509063444108]
Precision_mean:0.4525540275049116
Recall_mean:0.41866593965830606
AP_mean:0.39618962810060837
F1_mean:0.4349509063444108
computer mAP...
computer PR...
epoch:0070
Precision:[0.5471456061577935]
Recall:[0.3875863322428208]
AP:[0.4115535110030657]
F1:[0.45374753976275334]
Precision_mean:0.5471456061577935
Recall_mean:0.3875863322428208
AP_mean:0.4115535110030657
F1_mean:0.45374753976275334
computer mAP...
computer PR...
epoch:0080
Precision:[0.4435028248587571]
Recall:[0.42802617230098144]
AP:[0.4018588720863394]
F1:[0.4356270810210876]
Precision_mean:0.4435028248587571
Recall_mean:0.42802617230098144
AP_mean:0.4018588720863394
F1_mean:0.4356270810210876
computer mAP...
computer PR...
epoch:0090
Precision:[0.47763424789153414]
Recall:[0.4065794256633951]
AP:[0.4013335521855797]
F1:[0.43925187766923557]
Precision_mean:0.47763424789153414
Recall_mean:0.4065794256633951
AP_mean:0.4013335521855797
F1_mean:0.43925187766923557
computer mAP...
computer PR...
epoch:0100
Precision:[0.46715328467153283]
Recall:[0.4245728825881498]
AP:[0.4140082509800624]
F1:[0.4448464651273506]
Precision_mean:0.46715328467153283
Recall_mean:0.4245728825881498
AP_mean:0.4140082509800624
F1_mean:0.4448464651273506
computer mAP...
computer PR...
epoch:0110
Precision:[0.45037953331459096]
Recall:[0.4367502726281352]
AP:[0.41200528346180015]
F1:[0.4434602076124567]
Precision_mean:0.45037953331459096
Recall_mean:0.4367502726281352
AP_mean:0.41200528346180015
F1_mean:0.4434602076124567
computer mAP...
computer PR...
epoch:0120
Precision:[0.48530664395229983]
Recall:[0.41421301344965467]
AP:[0.4117045303096744]
F1:[0.446950382427927]
Precision_mean:0.48530664395229983
Recall_mean:0.41421301344965467
AP_mean:0.4117045303096744
F1_mean:0.446950382427927
computer mAP...
computer PR...
epoch:0130
Precision:[0.49516908212560384]
Recall:[0.4098509632860778]
AP:[0.412898851299066]
F1:[0.4484884645982497]
Precision_mean:0.49516908212560384
Recall_mean:0.4098509632860778
AP_mean:0.412898851299066
F1_mean:0.4484884645982497
computer mAP...
computer PR...
epoch:0140
Precision:[0.480049109883364]
Recall:[0.42639040348964014]
AP:[0.42418272096935744]
F1:[0.45163153335258444]
Precision_mean:0.480049109883364
Recall_mean:0.42639040348964014
AP_mean:0.42418272096935744
F1_mean:0.45163153335258444
computer mAP...
computer PR...
epoch:0150
Precision:[0.4711266919855877]
Recall:[0.4396583060705198]
AP:[0.42913084434274434]
F1:[0.4548488694589385]
Precision_mean:0.4711266919855877
Recall_mean:0.4396583060705198
AP_mean:0.42913084434274434
F1_mean:0.4548488694589385
computer mAP...
computer PR...
epoch:0160
Precision:[0.4483695652173913]
Recall:[0.44983642311886585]
AP:[0.4292368153846179]
F1:[0.4491017964071855]
Precision_mean:0.4483695652173913
Recall_mean:0.44983642311886585
AP_mean:0.4292368153846179
F1_mean:0.4491017964071855
computer mAP...
computer PR...
epoch:0170
Precision:[0.45366438988369945]
Recall:[0.44665576154125775]
AP:[0.4265991263357023]
F1:[0.45013279604359363]
Precision_mean:0.45366438988369945
Recall_mean:0.44665576154125775
AP_mean:0.4265991263357023
F1_mean:0.45013279604359363
computer mAP...
computer PR...
epoch:0180
Precision:[0.4429548317437466]
Recall:[0.4473827699018539]
AP:[0.426357397883935]
F1:[0.44515779003526534]
Precision_mean:0.4429548317437466
Recall_mean:0.4473827699018539
AP_mean:0.426357397883935
F1_mean:0.44515779003526534
computer mAP...
computer PR...
epoch:0190
Precision:[0.44763729246487866]
Recall:[0.44592875318066155]
AP:[0.42597229999666714]
F1:[0.44678138942001266]
Precision_mean:0.44763729246487866
Recall_mean:0.44592875318066155
AP_mean:0.42597229999666714
F1_mean:0.44678138942001266
computer mAP...
computer PR...
epoch:0200
Precision:[0.44929136756856247]
Recall:[0.4436568520537986]
AP:[0.4243574053880059]
F1:[0.44645633287608594]
Precision_mean:0.44929136756856247
Recall_mean:0.4436568520537986
AP_mean:0.4243574053880059
F1_mean:0.44645633287608594
computer mAP...
computer PR...
epoch:0210
Precision:[0.43712680014049876]
Recall:[0.4523809523809524]
AP:[0.4249032112852242]
F1:[0.4446230796713111]
Precision_mean:0.43712680014049876
Recall_mean:0.4523809523809524
AP_mean:0.4249032112852242
F1_mean:0.4446230796713111
computer mAP...
computer PR...
epoch:0220
Precision:[0.4560352092892593]
Recall:[0.4425663395129044]
AP:[0.42683952863333086]
F1:[0.44919983397131386]
Precision_mean:0.4560352092892593
Recall_mean:0.4425663395129044
AP_mean:0.42683952863333086
F1_mean:0.44919983397131386
computer mAP...
computer PR...
epoch:0230
Precision:[0.4507610489638731]
Recall:[0.44674663758633226]
AP:[0.42483634758036665]
F1:[0.4487448653582838]
Precision_mean:0.4507610489638731
Recall_mean:0.44674663758633226
AP_mean:0.42483634758036665
F1_mean:0.4487448653582838
computer mAP...
computer PR...
epoch:0240
Precision:[0.4369563307266497]
Recall:[0.4519265721555798]
AP:[0.42429923222384786]
F1:[0.4443153897699352]
Precision_mean:0.4369563307266497
Recall_mean:0.4519265721555798
AP_mean:0.42429923222384786
F1_mean:0.4443153897699352
computer mAP...
computer PR...
epoch:0250
Precision:[0.4540977513473332]
Recall:[0.4441112322791712]
AP:[0.42472714459277405]
F1:[0.4490489754663236]
Precision_mean:0.4540977513473332
Recall_mean:0.4441112322791712
AP_mean:0.42472714459277405
F1_mean:0.4490489754663236
computer mAP...
computer PR...
epoch:0260
Precision:[0.4454758471521269]
Recall:[0.4492002908033442]
AP:[0.42547405397642224]
F1:[0.4473303167420814]
Precision_mean:0.4454758471521269
Recall_mean:0.4492002908033442
AP_mean:0.42547405397642224
F1_mean:0.4473303167420814
computer mAP...
computer PR...
epoch:0270
Precision:[0.44767654592023415]
Recall:[0.44474736459469283]
AP:[0.424264210990639]
F1:[0.4462071480671042]
Precision_mean:0.44767654592023415
Recall_mean:0.44474736459469283
AP_mean:0.424264210990639
F1_mean:0.4462071480671042
computer mAP...
computer PR...
epoch:0280
Precision:[0.44237485448195574]
Recall:[0.4489276626681207]
AP:[0.4236999105868655]
F1:[0.44562717062829815]
Precision_mean:0.44237485448195574
Recall_mean:0.4489276626681207
AP_mean:0.4236999105868655
F1_mean:0.44562717062829815
computer mAP...
computer PR...
epoch:0290
Precision:[0.4449795547478419]
Recall:[0.4450199927299164]
AP:[0.4233275058738515]
F1:[0.44499977282020986]
Precision_mean:0.4449795547478419
Recall_mean:0.4450199927299164
AP_mean:0.4233275058738515
F1_mean:0.44499977282020986
