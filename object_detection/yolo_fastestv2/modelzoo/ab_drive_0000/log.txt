训练配置:
{'model_name': 'abd', 'epochs': 300, 'steps': [150.0, 250.0], 'batch_size': 128, 'subdivisions': 1, 'learning_rate': 0.001, 'pre_weights': 'None', 'classes': 5, 'width': 256, 'height': 256, 'anchor_num': 3, 'separation': 4, 'separation_scale': 2, 'anchors': [16.33, 6.18, 24.36, 2.35, 30.22, 104.04, 33.22, 10.2, 74.09, 96.71, 128.15, 107.99], 'conf_thr': 0.01, 'nms_thr': 0.4, 'iou_thr': 0.5, 'label_flag': '', 'val': '../../../datasets/abnormal_drive_0/val0.txt', 'train': '../../../datasets/abnormal_drive_0/train0.txt', 'names': './configs/ab_drive.names'}
load param...
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 24, 64, 64]             648
       BatchNorm2d-2           [-1, 24, 64, 64]              48
              ReLU-3           [-1, 24, 64, 64]               0
         MaxPool2d-4           [-1, 24, 32, 32]               0
            Conv2d-5           [-1, 24, 16, 16]             216
       BatchNorm2d-6           [-1, 24, 16, 16]              48
            Conv2d-7           [-1, 24, 16, 16]             576
       BatchNorm2d-8           [-1, 24, 16, 16]              48
              ReLU-9           [-1, 24, 16, 16]               0
           Conv2d-10           [-1, 24, 32, 32]             576
      BatchNorm2d-11           [-1, 24, 32, 32]              48
             ReLU-12           [-1, 24, 32, 32]               0
           Conv2d-13           [-1, 24, 16, 16]             216
      BatchNorm2d-14           [-1, 24, 16, 16]              48
           Conv2d-15           [-1, 24, 16, 16]             576
      BatchNorm2d-16           [-1, 24, 16, 16]              48
             ReLU-17           [-1, 24, 16, 16]               0
  ShuffleV2Block_-18           [-1, 48, 16, 16]               0
           Conv2d-19           [-1, 24, 16, 16]             576
      BatchNorm2d-20           [-1, 24, 16, 16]              48
             ReLU-21           [-1, 24, 16, 16]               0
           Conv2d-22           [-1, 24, 16, 16]             216
      BatchNorm2d-23           [-1, 24, 16, 16]              48
           Conv2d-24           [-1, 24, 16, 16]             576
      BatchNorm2d-25           [-1, 24, 16, 16]              48
             ReLU-26           [-1, 24, 16, 16]               0
  ShuffleV2Block_-27           [-1, 48, 16, 16]               0
           Conv2d-28           [-1, 24, 16, 16]             576
      BatchNorm2d-29           [-1, 24, 16, 16]              48
             ReLU-30           [-1, 24, 16, 16]               0
           Conv2d-31           [-1, 24, 16, 16]             216
      BatchNorm2d-32           [-1, 24, 16, 16]              48
           Conv2d-33           [-1, 24, 16, 16]             576
      BatchNorm2d-34           [-1, 24, 16, 16]              48
             ReLU-35           [-1, 24, 16, 16]               0
  ShuffleV2Block_-36           [-1, 48, 16, 16]               0
           Conv2d-37           [-1, 24, 16, 16]             576
      BatchNorm2d-38           [-1, 24, 16, 16]              48
             ReLU-39           [-1, 24, 16, 16]               0
           Conv2d-40           [-1, 24, 16, 16]             216
      BatchNorm2d-41           [-1, 24, 16, 16]              48
           Conv2d-42           [-1, 24, 16, 16]             576
      BatchNorm2d-43           [-1, 24, 16, 16]              48
             ReLU-44           [-1, 24, 16, 16]               0
  ShuffleV2Block_-45           [-1, 48, 16, 16]               0
           Conv2d-46             [-1, 48, 8, 8]             432
      BatchNorm2d-47             [-1, 48, 8, 8]              96
           Conv2d-48             [-1, 48, 8, 8]           2,304
      BatchNorm2d-49             [-1, 48, 8, 8]              96
             ReLU-50             [-1, 48, 8, 8]               0
           Conv2d-51           [-1, 48, 16, 16]           2,304
      BatchNorm2d-52           [-1, 48, 16, 16]              96
             ReLU-53           [-1, 48, 16, 16]               0
           Conv2d-54             [-1, 48, 8, 8]             432
      BatchNorm2d-55             [-1, 48, 8, 8]              96
           Conv2d-56             [-1, 48, 8, 8]           2,304
      BatchNorm2d-57             [-1, 48, 8, 8]              96
             ReLU-58             [-1, 48, 8, 8]               0
  ShuffleV2Block_-59             [-1, 96, 8, 8]               0
           Conv2d-60             [-1, 48, 8, 8]           2,304
      BatchNorm2d-61             [-1, 48, 8, 8]              96
             ReLU-62             [-1, 48, 8, 8]               0
           Conv2d-63             [-1, 48, 8, 8]             432
      BatchNorm2d-64             [-1, 48, 8, 8]              96
           Conv2d-65             [-1, 48, 8, 8]           2,304
      BatchNorm2d-66             [-1, 48, 8, 8]              96
             ReLU-67             [-1, 48, 8, 8]               0
  ShuffleV2Block_-68             [-1, 96, 8, 8]               0
           Conv2d-69             [-1, 48, 8, 8]           2,304
      BatchNorm2d-70             [-1, 48, 8, 8]              96
             ReLU-71             [-1, 48, 8, 8]               0
           Conv2d-72             [-1, 48, 8, 8]             432
      BatchNorm2d-73             [-1, 48, 8, 8]              96
           Conv2d-74             [-1, 48, 8, 8]           2,304
      BatchNorm2d-75             [-1, 48, 8, 8]              96
             ReLU-76             [-1, 48, 8, 8]               0
  ShuffleV2Block_-77             [-1, 96, 8, 8]               0
           Conv2d-78             [-1, 48, 8, 8]           2,304
      BatchNorm2d-79             [-1, 48, 8, 8]              96
             ReLU-80             [-1, 48, 8, 8]               0
           Conv2d-81             [-1, 48, 8, 8]             432
      BatchNorm2d-82             [-1, 48, 8, 8]              96
           Conv2d-83             [-1, 48, 8, 8]           2,304
      BatchNorm2d-84             [-1, 48, 8, 8]              96
             ReLU-85             [-1, 48, 8, 8]               0
  ShuffleV2Block_-86             [-1, 96, 8, 8]               0
           Conv2d-87             [-1, 48, 8, 8]           2,304
      BatchNorm2d-88             [-1, 48, 8, 8]              96
             ReLU-89             [-1, 48, 8, 8]               0
           Conv2d-90             [-1, 48, 8, 8]             432
      BatchNorm2d-91             [-1, 48, 8, 8]              96
           Conv2d-92             [-1, 48, 8, 8]           2,304
      BatchNorm2d-93             [-1, 48, 8, 8]              96
             ReLU-94             [-1, 48, 8, 8]               0
  ShuffleV2Block_-95             [-1, 96, 8, 8]               0
           Conv2d-96             [-1, 48, 8, 8]           2,304
      BatchNorm2d-97             [-1, 48, 8, 8]              96
             ReLU-98             [-1, 48, 8, 8]               0
           Conv2d-99             [-1, 48, 8, 8]             432
     BatchNorm2d-100             [-1, 48, 8, 8]              96
          Conv2d-101             [-1, 48, 8, 8]           2,304
     BatchNorm2d-102             [-1, 48, 8, 8]              96
            ReLU-103             [-1, 48, 8, 8]               0
 ShuffleV2Block_-104             [-1, 96, 8, 8]               0
          Conv2d-105             [-1, 48, 8, 8]           2,304
     BatchNorm2d-106             [-1, 48, 8, 8]              96
            ReLU-107             [-1, 48, 8, 8]               0
          Conv2d-108             [-1, 48, 8, 8]             432
     BatchNorm2d-109             [-1, 48, 8, 8]              96
          Conv2d-110             [-1, 48, 8, 8]           2,304
     BatchNorm2d-111             [-1, 48, 8, 8]              96
            ReLU-112             [-1, 48, 8, 8]               0
 ShuffleV2Block_-113             [-1, 96, 8, 8]               0
          Conv2d-114             [-1, 48, 8, 8]           2,304
     BatchNorm2d-115             [-1, 48, 8, 8]              96
            ReLU-116             [-1, 48, 8, 8]               0
          Conv2d-117             [-1, 48, 8, 8]             432
     BatchNorm2d-118             [-1, 48, 8, 8]              96
          Conv2d-119             [-1, 48, 8, 8]           2,304
     BatchNorm2d-120             [-1, 48, 8, 8]              96
            ReLU-121             [-1, 48, 8, 8]               0
 ShuffleV2Block_-122             [-1, 96, 8, 8]               0
          Conv2d-123             [-1, 96, 8, 8]             864
     BatchNorm2d-124             [-1, 96, 8, 8]             192
          Conv2d-125             [-1, 96, 8, 8]           9,216
     BatchNorm2d-126             [-1, 96, 8, 8]             192
            ReLU-127             [-1, 96, 8, 8]               0
          Conv2d-128           [-1, 96, 16, 16]           9,216
     BatchNorm2d-129           [-1, 96, 16, 16]             192
            ReLU-130           [-1, 96, 16, 16]               0
          Conv2d-131             [-1, 96, 8, 8]             864
     BatchNorm2d-132             [-1, 96, 8, 8]             192
          Conv2d-133             [-1, 96, 8, 8]           9,216
     BatchNorm2d-134             [-1, 96, 8, 8]             192
            ReLU-135             [-1, 96, 8, 8]               0
 ShuffleV2Block_-136            [-1, 192, 8, 8]               0
          Conv2d-137             [-1, 96, 8, 8]           9,216
     BatchNorm2d-138             [-1, 96, 8, 8]             192
            ReLU-139             [-1, 96, 8, 8]               0
          Conv2d-140             [-1, 96, 8, 8]             864
     BatchNorm2d-141             [-1, 96, 8, 8]             192
          Conv2d-142             [-1, 96, 8, 8]           9,216
     BatchNorm2d-143             [-1, 96, 8, 8]             192
            ReLU-144             [-1, 96, 8, 8]               0
 ShuffleV2Block_-145            [-1, 192, 8, 8]               0
          Conv2d-146             [-1, 96, 8, 8]           9,216
     BatchNorm2d-147             [-1, 96, 8, 8]             192
            ReLU-148             [-1, 96, 8, 8]               0
          Conv2d-149             [-1, 96, 8, 8]             864
     BatchNorm2d-150             [-1, 96, 8, 8]             192
          Conv2d-151             [-1, 96, 8, 8]           9,216
     BatchNorm2d-152             [-1, 96, 8, 8]             192
            ReLU-153             [-1, 96, 8, 8]               0
 ShuffleV2Block_-154            [-1, 192, 8, 8]               0
          Conv2d-155             [-1, 96, 8, 8]           9,216
     BatchNorm2d-156             [-1, 96, 8, 8]             192
            ReLU-157             [-1, 96, 8, 8]               0
          Conv2d-158             [-1, 96, 8, 8]             864
     BatchNorm2d-159             [-1, 96, 8, 8]             192
          Conv2d-160             [-1, 96, 8, 8]           9,216
     BatchNorm2d-161             [-1, 96, 8, 8]             192
            ReLU-162             [-1, 96, 8, 8]               0
 ShuffleV2Block_-163            [-1, 192, 8, 8]               0
  ShuffleNetV2Sp-164  [[-1, 96, 16, 16], [-1, 192, 8, 8]]               0
          Conv2d-165             [-1, 72, 8, 8]          13,824
     BatchNorm2d-166             [-1, 72, 8, 8]             144
            ReLU-167             [-1, 72, 8, 8]               0
          Conv2d-168             [-1, 72, 8, 8]           1,800
     BatchNorm2d-169             [-1, 72, 8, 8]             144
            ReLU-170             [-1, 72, 8, 8]               0
          Conv2d-171             [-1, 72, 8, 8]           5,184
     BatchNorm2d-172             [-1, 72, 8, 8]             144
          Conv2d-173             [-1, 72, 8, 8]           1,800
     BatchNorm2d-174             [-1, 72, 8, 8]             144
            ReLU-175             [-1, 72, 8, 8]               0
          Conv2d-176             [-1, 72, 8, 8]           5,184
     BatchNorm2d-177             [-1, 72, 8, 8]             144
     DWConvblock-178             [-1, 72, 8, 8]               0
          Conv2d-179             [-1, 72, 8, 8]           1,800
     BatchNorm2d-180             [-1, 72, 8, 8]             144
            ReLU-181             [-1, 72, 8, 8]               0
          Conv2d-182             [-1, 72, 8, 8]           5,184
     BatchNorm2d-183             [-1, 72, 8, 8]             144
          Conv2d-184             [-1, 72, 8, 8]           1,800
     BatchNorm2d-185             [-1, 72, 8, 8]             144
            ReLU-186             [-1, 72, 8, 8]               0
          Conv2d-187             [-1, 72, 8, 8]           5,184
     BatchNorm2d-188             [-1, 72, 8, 8]             144
     DWConvblock-189             [-1, 72, 8, 8]               0
          Conv2d-190           [-1, 72, 16, 16]          20,736
     BatchNorm2d-191           [-1, 72, 16, 16]             144
            ReLU-192           [-1, 72, 16, 16]               0
          Conv2d-193           [-1, 72, 16, 16]           1,800
     BatchNorm2d-194           [-1, 72, 16, 16]             144
            ReLU-195           [-1, 72, 16, 16]               0
          Conv2d-196           [-1, 72, 16, 16]           5,184
     BatchNorm2d-197           [-1, 72, 16, 16]             144
          Conv2d-198           [-1, 72, 16, 16]           1,800
     BatchNorm2d-199           [-1, 72, 16, 16]             144
            ReLU-200           [-1, 72, 16, 16]               0
          Conv2d-201           [-1, 72, 16, 16]           5,184
     BatchNorm2d-202           [-1, 72, 16, 16]             144
     DWConvblock-203           [-1, 72, 16, 16]               0
          Conv2d-204           [-1, 72, 16, 16]           1,800
     BatchNorm2d-205           [-1, 72, 16, 16]             144
            ReLU-206           [-1, 72, 16, 16]               0
          Conv2d-207           [-1, 72, 16, 16]           5,184
     BatchNorm2d-208           [-1, 72, 16, 16]             144
          Conv2d-209           [-1, 72, 16, 16]           1,800
     BatchNorm2d-210           [-1, 72, 16, 16]             144
            ReLU-211           [-1, 72, 16, 16]               0
          Conv2d-212           [-1, 72, 16, 16]           5,184
     BatchNorm2d-213           [-1, 72, 16, 16]             144
     DWConvblock-214           [-1, 72, 16, 16]               0
        LightFPN-215  [[-1, 72, 16, 16], [-1, 72, 16, 16], [-1, 72, 16, 16], [-1, 72, 8, 8], [-1, 72, 8, 8], [-1, 72, 8, 8]]               0
          Conv2d-216           [-1, 12, 16, 16]             876
          Conv2d-217            [-1, 3, 16, 16]             219
          Conv2d-218            [-1, 5, 16, 16]             365
          Conv2d-219             [-1, 12, 8, 8]             876
          Conv2d-220              [-1, 3, 8, 8]             219
          Conv2d-221              [-1, 5, 8, 8]             365
================================================================
Total params: 239,080
Trainable params: 239,080
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.75
Forward/backward pass size (MB): 2289.85
Params size (MB): 0.91
Estimated Total Size (MB): 2291.52
----------------------------------------------------------------
Starting training for 300 epochs...
computer mAP...
computer PR...
epoch:0010
Precision:[0.20512820512820512, 0.5332678073536197, 0.24764089121887287, 0.05263157894736842, 0.7675965785207134]
Recall:[0.034594594594594595, 0.7831556000707005, 0.432974335472044, 0.005479452054794521, 0.7758377126066565]
AP:[0.012301881809631644, 0.6648791375465959, 0.19733990721957345, 0.0037673466834660307, 0.7863792436050907]
F1:[0.059204440333024945, 0.6344944450411103, 0.31507420376855083, 0.00992555831265507, 0.7716951438848921]
Precision_mean:0.36125301223375594
Recall_mean:0.40640833895975803
AP_mean:0.33293350337287153
F1_mean:0.35807875826804664
computer mAP...
computer PR...
epoch:0020
Precision:[0.2611336032388664, 0.6688787661571901, 0.3545869590666094, 0.14919354838709678, 0.8716377851683296]
Recall:[0.13945945945945945, 0.8521475284275025, 0.5205659945004583, 0.10136986301369863, 0.8441543764479855]
AP:[0.06627885671259963, 0.7963336528256629, 0.31752763440481513, 0.035081949102270846, 0.8697752667600563]
F1:[0.1818181818181818, 0.749472102392704, 0.4218369194345797, 0.12071778140293633, 0.8576759673900564]
Precision_mean:0.46108613240361845
Recall_mean:0.4915394443698209
AP_mean:0.416999471961081
F1_mean:0.4663041904876916
computer mAP...
computer PR...
epoch:0030
Precision:[0.16468435498627632, 0.7264528562957107, 0.4393894043699491, 0.12283236994219653, 0.8954359274429491]
Recall:[0.1945945945945946, 0.8661108819890414, 0.5886801099908341, 0.2328767123287671, 0.8647228343787082]
AP:[0.0665132545270337, 0.8163614166792226, 0.42273575463284335, 0.08695616311955828, 0.8905077251078249]
F1:[0.17839444995044595, 0.7901582950361472, 0.503195161961658, 0.16083254493850516, 0.8798114238077442]
Precision_mean:0.4697589826074163
Recall_mean:0.5493970266563891
AP_mean:0.4566148628132966
F1_mean:0.5024783751389001
computer mAP...
computer PR...
epoch:0040
Precision:[0.24662576687116564, 0.7517096928861635, 0.48764817663559856, 0.23204419889502761, 0.9126127719794799]
Recall:[0.2172972972972973, 0.8775113415424498, 0.6174381301558204, 0.11506849315068493, 0.8745550093236142]
AP:[0.10371006023095998, 0.8441272010080393, 0.4608874332620858, 0.055105190664080525, 0.9016535519448869]
F1:[0.2310344827586206, 0.8097535780789148, 0.5449213812629556, 0.1538461538461538, 0.8931786703601108]
Precision_mean:0.526128121453487
Recall_mean:0.5403740542939733
AP_mean:0.4730966874220105
F1_mean:0.5265468532613511
computer mAP...
computer PR...
epoch:0050
Precision:[0.2615844544095665, 0.764462179747865, 0.5084823686267298, 0.21785714285714286, 0.9186518440091759]
Recall:[0.1891891891891892, 0.8860248630177341, 0.6335930339138405, 0.16712328767123288, 0.8825224614341414]
AP:[0.07964505215263051, 0.8541999245639668, 0.4958929747659052, 0.0760411078425628, 0.9093423651434069]
F1:[0.219573400250941, 0.8207668167553555, 0.5641849669701838, 0.1891472868217054, 0.9002247968182604]
Precision_mean:0.5342075979300961
Recall_mean:0.5516905670452277
AP_mean:0.4830242848936944
F1_mean:0.5387794535232893
computer mAP...
computer PR...
epoch:0060
Precision:[0.21435059037238874, 0.791466456610214, 0.5226570545829042, 0.20303030303030303, 0.9126528719642961]
Recall:[0.25513513513513514, 0.8879691274376952, 0.6396081576535289, 0.18356164383561643, 0.8897553257614285]
AP:[0.10055342479708218, 0.8580637875847456, 0.497883378958915, 0.08831065799576635, 0.9153109794024398]
F1:[0.2329713721618953, 0.8369452041482139, 0.5752485960121593, 0.1928057553956834, 0.9010586552217454]
Precision_mean:0.5288314553120212
Recall_mean:0.5712058779646808
AP_mean:0.49202444574778975
F1_mean:0.5478059165879394
computer mAP...
computer PR...
epoch:0070
Precision:[0.2502870264064294, 0.7883608905090483, 0.5456313442747367, 0.25, 0.9073628764456658]
Recall:[0.23567567567567568, 0.8919165733812526, 0.6589711274060495, 0.14520547945205478, 0.8955190145222354]
AP:[0.10101162514833104, 0.8640300029339811, 0.5257641151520704, 0.06015793408895076, 0.9162560391549437]
F1:[0.242761692650334, 0.8369476579452945, 0.5969692251803415, 0.18370883882149042, 0.9014020419190626]
Precision_mean:0.548328427527176
Recall_mean:0.5654575740874537
AP_mean:0.49344394329565533
F1_mean:0.5523578913033046
computer mAP...
computer PR...
epoch:0080
Precision:[0.3003875968992248, 0.7831433329063661, 0.5590915597118183, 0.1729957805907173, 0.9147812105202711]
Recall:[0.16756756756756758, 0.9005479290638072, 0.6712878093492209, 0.22465753424657534, 0.9001525682319037]
AP:[0.09299801791129506, 0.877454244773213, 0.5551273171494192, 0.07424835842362514, 0.9218759508221365]
F1:[0.2151283830673143, 0.8377522916923583, 0.6100741897696214, 0.19547079856972582, 0.907407934835237]
Precision_mean:0.5460798961256794
Recall_mean:0.572842681691815
AP_mean:0.5043407778159377
F1_mean:0.5531667195868513
computer mAP...
computer PR...
epoch:0090
Precision:[0.27827648114901254, 0.8072263717701708, 0.5735566157141466, 0.21875, 0.9191249199976727]
Recall:[0.16756756756756758, 0.9009898073410711, 0.6749541704857929, 0.23013698630136986, 0.892637170141832]
AP:[0.09785853001731733, 0.8812607560740602, 0.552273766704574, 0.08033345625188464, 0.9185044542885423]
F1:[0.2091767881241565, 0.8515347671747755, 0.6201379019948419, 0.2242990654205607, 0.9056874211672973]
Precision_mean:0.5593868777262005
Recall_mean:0.5732571403675266
AP_mean:0.5060461926672757
F1_mean:0.5621671887763264
computer mAP...
computer PR...
epoch:0100
Precision:[0.24057649667405764, 0.8103091690088561, 0.5765205371248026, 0.1806853582554517, 0.9136715391229578]
Recall:[0.23459459459459459, 0.9002533435456312, 0.6689963336388635, 0.3178082191780822, 0.9006611290049161]
AP:[0.10517278594162711, 0.8805041091198235, 0.5571336358674926, 0.13308262759679101, 0.9230190547850827]
F1:[0.23754789272030646, 0.8529165503767792, 0.6193254136614341, 0.23038728897715982, 0.9071196858459962]
Precision_mean:0.5443526200372252
Recall_mean:0.6044627239924175
AP_mean:0.5197824426621633
F1_mean:0.5694593663163351
computer mAP...
computer PR...
epoch:0110
Precision:[0.26716738197424894, 0.8160996338562685, 0.5755603406087892, 0.189873417721519, 0.9212233549582948]
Recall:[0.2691891891891892, 0.899546338302009, 0.6737511457378552, 0.1232876712328767, 0.8986833926654235]
AP:[0.13756166997628144, 0.8824843539999403, 0.5544607634027708, 0.04214229224786079, 0.9229134914800794]
F1:[0.2681744749596122, 0.8557936185418213, 0.6207970440749538, 0.14950166112956803, 0.9098137925116558]
Precision_mean:0.553984825823824
Recall_mean:0.5728915474254708
AP_mean:0.5079125142213865
F1_mean:0.5608161182435223
computer mAP...
computer PR...
epoch:0120
Precision:[0.3213675213675214, 0.8158907016845736, 0.5973472983555208, 0.24305555555555555, 0.9173653382336051]
Recall:[0.20324324324324325, 0.9059977611500618, 0.6991865261228231, 0.1917808219178082, 0.9026953720969656]
AP:[0.11488189013530203, 0.8906714583980234, 0.5957954084977132, 0.08557336486351019, 0.9248178417388214]
F1:[0.24900662251655628, 0.8585865635599725, 0.6442673141891891, 0.21439509954058186, 0.9099712340861839]
Precision_mean:0.5790052830393553
Recall_mean:0.5805807449061804
AP_mean:0.522347992726674
F1_mean:0.5752453667784968
computer mAP...
computer PR...
epoch:0130
Precision:[0.2550335570469799, 0.8168062327750689, 0.5760146534271667, 0.3160377358490566, 0.9325770422782294]
Recall:[0.20540540540540542, 0.9080009426736582, 0.6845783684692942, 0.18356164383561643, 0.9011696897779284]
AP:[0.11713195556819307, 0.8906862818378309, 0.5694215196867533, 0.1032470779532618, 0.9260769462034316]
F1:[0.22754491017964068, 0.8599927457381211, 0.6256216951992043, 0.2322357019064124, 0.9166044025518708]
Precision_mean:0.5792938442753004
Recall_mean:0.5765432100323806
AP_mean:0.5213127562498941
F1_mean:0.5723998911150499
computer mAP...
computer PR...
epoch:0140
Precision:[0.28526148969889065, 0.8281447939408642, 0.5974752475247525, 0.27621483375959077, 0.9299266674426726]
Recall:[0.1945945945945946, 0.905114004595534, 0.6913955087076077, 0.2958904109589041, 0.9028648923546364]
AP:[0.1114264034255406, 0.8926899121182772, 0.5831825701545033, 0.14210241435437487, 0.9263601042267786]
F1:[0.23136246786632386, 0.8649204048024548, 0.6410133843212238, 0.28571428571428564, 0.9161959918575647]
Precision_mean:0.5834046064733541
Recall_mean:0.5979718822422554
AP_mean:0.5311522808558948
F1_mean:0.5878413069123705
computer mAP...
computer PR...
epoch:0150
Precision:[0.33488372093023255, 0.8338047899997306, 0.6450161100598374, 0.2857142857142857, 0.9388674388674388]
Recall:[0.23351351351351352, 0.9117421787544925, 0.7225022914757103, 0.2191780821917808, 0.9068768717861785]
AP:[0.12338349767666612, 0.8972966601684473, 0.6268181407010263, 0.12564170041349354, 0.9305533135752476]
F1:[0.27515923566878975, 0.8710335608245973, 0.6815639439055364, 0.2480620155038759, 0.9225949239745912]
Precision_mean:0.607657269114305
Recall_mean:0.5987625875443351
AP_mean:0.5407386625069762
F1_mean:0.5996827359754782
computer mAP...
computer PR...
epoch:0160
Precision:[0.338006230529595, 0.8379963776930771, 0.6529983043009095, 0.32061068702290074, 0.9395581978201317]
Recall:[0.23459459459459459, 0.9132151063453721, 0.7280018331805683, 0.23013698630136986, 0.9108888512177206]
AP:[0.135485257178671, 0.8981224774359586, 0.638156361463728, 0.1317230297925994, 0.9322508823821415]
F1:[0.27696234843650275, 0.8739903297199002, 0.6884633095863695, 0.26794258373205737, 0.9250014345555747]
Precision_mean:0.6178339594733229
Recall_mean:0.6033674743279251
AP_mean:0.5471476016506196
F1_mean:0.6064720012060809
computer mAP...
computer PR...
epoch:0170
Precision:[0.328125, 0.8404405381944444, 0.6554384160452559, 0.3320754716981132, 0.9407086890422173]
Recall:[0.22702702702702704, 0.9126848524126554, 0.7301214482126489, 0.2410958904109589, 0.9090806351358988]
AP:[0.12441333737954416, 0.8989683890443796, 0.6389680064444051, 0.13596081153939565, 0.9317102073086494]
F1:[0.2683706070287539, 0.8750741420703291, 0.6907671878810872, 0.27936507936507926, 0.9246242708123796]
Precision_mean:0.6193576229960062
Recall_mean:0.6040019706398378
AP_mean:0.5460041503432749
F1_mean:0.6076402574315258
computer mAP...
computer PR...
epoch:0180
Precision:[0.3165905631659056, 0.8390071113754969, 0.6563676690565843, 0.3181818181818182, 0.9372160745486313]
Recall:[0.22486486486486487, 0.9140694043480823, 0.7289757103574702, 0.2493150684931507, 0.9093066621461264]
AP:[0.12890176970207917, 0.8989282578632395, 0.6369797792445158, 0.13719795588243586, 0.9310163387180339]
F1:[0.2629582806573956, 0.8749312692974666, 0.6907689384686371, 0.2795698924731182, 0.9230504488484813]
Precision_mean:0.6134726472656873
Recall_mean:0.6053063420419389
AP_mean:0.5466048202820609
F1_mean:0.6062557659490198
computer mAP...
computer PR...
epoch:0190
Precision:[0.3408723747980614, 0.8400799330290838, 0.6513306038894575, 0.2921686746987952, 0.9396094433109881]
Recall:[0.2281081081081081, 0.9164260884934896, 0.729090284142988, 0.26575342465753427, 0.9108323444651636]
AP:[0.12887936136274547, 0.9017458471199968, 0.6369994737106043, 0.14345660566329968, 0.931957250872886]
F1:[0.2733160621761657, 0.8765938261690407, 0.6880203265217861, 0.2783357245337159, 0.9249971307242052]
Precision_mean:0.6128122059452771
Recall_mean:0.6100420499734567
AP_mean:0.5486077077459065
F1_mean:0.6082526140249828
computer mAP...
computer PR...
epoch:0200
Precision:[0.32, 0.8435783060436273, 0.6577874141758299, 0.3184931506849315, 0.9408463876548983]
Recall:[0.22486486486486487, 0.913656984622636, 0.7299495875343721, 0.2547945205479452, 0.9095326891563542]
AP:[0.12383474192396257, 0.8990584147611493, 0.6380832733677129, 0.1498413488526059, 0.9314008374971575]
F1:[0.2641269841269841, 0.8772202737866275, 0.6919922882667608, 0.28310502283105016, 0.9249245798017527]
Precision_mean:0.6161410517118574
Recall_mean:0.6065597293452345
AP_mean:0.5484437232805176
F1_mean:0.608273829762635
computer mAP...
computer PR...
epoch:0210
Precision:[0.33533834586466166, 0.8460533122770714, 0.6586684516760208, 0.3233333333333333, 0.9392313524231644]
Recall:[0.2410810810810811, 0.9153655806280563, 0.7328139321723189, 0.26575342465753427, 0.9100412499293665]
AP:[0.13912831588315583, 0.9031349254261114, 0.6429060551027536, 0.14887056554281203, 0.9317648004459026]
F1:[0.280503144654088, 0.8793457190157485, 0.6937657618569841, 0.2917293233082706, 0.9244059235449431]
Precision_mean:0.6205249591148503
Recall_mean:0.6130110536936714
AP_mean:0.5531609324801471
F1_mean:0.613949974476007
computer mAP...
computer PR...
epoch:0220
Precision:[0.3323572474377745, 0.8432847984352928, 0.6557982159335589, 0.31962025316455694, 0.9399871442762812]
Recall:[0.2454054054054054, 0.9144818240735285, 0.7328139321723189, 0.27671232876712326, 0.9089676216307849]
AP:[0.13444330433090773, 0.900915383387423, 0.6436404379416023, 0.15770486065126027, 0.9309770570074752]
F1:[0.2823383084577114, 0.877441420051443, 0.6921703371029705, 0.2966226138032304, 0.9242171789715599]
Precision_mean:0.6182095318494929
Recall_mean:0.6156762224098322
AP_mean:0.5535362086637338
F1_mean:0.6145579716773831
computer mAP...
computer PR...
epoch:0230
Precision:[0.33170731707317075, 0.8409300308926345, 0.6644117187906541, 0.3106796116504854, 0.9399625862270549]
Recall:[0.22054054054054054, 0.914157780003535, 0.7314390467461045, 0.26301369863013696, 0.9085720743628863]
AP:[0.12859829658311692, 0.8986288807680838, 0.6410304293636524, 0.14804792449876034, 0.9302634088996515]
F1:[0.26493506493506486, 0.8760162601626017, 0.6963160908570337, 0.2848664688427299, 0.9240008045283452]
Precision_mean:0.6175382529267999
Recall_mean:0.6075446280566407
AP_mean:0.549313788022653
F1_mean:0.6092269378651551
computer mAP...
computer PR...
epoch:0240
Precision:[0.3205317577548006, 0.8454198473282443, 0.6548931152919465, 0.3041958041958042, 0.939438182561467]
Recall:[0.23459459459459459, 0.913509691863548, 0.731840054995417, 0.23835616438356164, 0.9089676216307849]
AP:[0.12902817616839798, 0.9003588252063865, 0.6390941803540552, 0.12964689876243374, 0.9312158190999584]
F1:[0.27091136079900113, 0.8781468580975844, 0.6912317723128534, 0.26728110599078336, 0.9239517518667433]
Precision_mean:0.6128957414264524
Recall_mean:0.6054536254935812
AP_mean:0.5458687799182463
F1_mean:0.6063045698133931
computer mAP...
computer PR...
epoch:0250
Precision:[0.3159379407616361, 0.8455337690631808, 0.6623760834587636, 0.31561461794019935, 0.9376820034944671]
Recall:[0.24216216216216216, 0.9146291168326165, 0.7310953253895509, 0.2602739726027397, 0.9097587161665819]
AP:[0.13332906171635922, 0.9009203414286665, 0.6400242534107178, 0.14832890764097792, 0.9307357735254019]
F1:[0.27417380660954704, 0.8787252709931225, 0.6950412547994445, 0.2852852852852852, 0.9235093354748043]
Precision_mean:0.6154288829436494
Recall_mean:0.6115838586307303
AP_mean:0.5506676675444246
F1_mean:0.6113469906324408
computer mAP...
computer PR...
epoch:0260
Precision:[0.3203463203463203, 0.845944029240085, 0.6626187139965748, 0.3286219081272085, 0.9387160666433648]
Recall:[0.24, 0.9136275260708183, 0.7314390467461045, 0.2547945205479452, 0.910549810702379]
AP:[0.13193276909984486, 0.9003587894787699, 0.6404580455121164, 0.14190299346917115, 0.9317363727510415]
F1:[0.27441285537700855, 0.8784840244731476, 0.6953301565690947, 0.287037037037037, 0.9244184378854373]
Precision_mean:0.6192494076707107
Recall_mean:0.6100821808134494
AP_mean:0.5492777940621888
F1_mean:0.611936502268345
computer mAP...
computer PR...
epoch:0270
Precision:[0.3216374269005848, 0.8457091434648633, 0.6625706434385856, 0.3263888888888889, 0.9416203568294823]
Recall:[0.23783783783783785, 0.9135980675190007, 0.7320692025664528, 0.25753424657534246, 0.9095891959089111]
AP:[0.13221938343838055, 0.9001869086514933, 0.6411206113901629, 0.14683501945944658, 0.9320632456600847]
F1:[0.27346177750155365, 0.8783437415919679, 0.6955882753177477, 0.28790199081163853, 0.9253276615313865]
Precision_mean:0.619585291904481
Recall_mean:0.610125710081509
AP_mean:0.5504850337199135
F1_mean:0.6121246893508588
computer mAP...
computer PR...
epoch:0280
Precision:[0.32122093023255816, 0.8472533479092649, 0.663478532071941, 0.3262411347517731, 0.9372202522815788]
Recall:[0.23891891891891892, 0.9132445648971896, 0.7312098991750687, 0.25205479452054796, 0.9110583714753913]
AP:[0.1332376184952198, 0.89977277090064, 0.6406473312147455, 0.13805493310259423, 0.9316934204092641]
F1:[0.27402355858648475, 0.8790121356470456, 0.695699569411893, 0.2843894899536321, 0.9239541547277936]
Precision_mean:0.6190828394494232
Recall_mean:0.6092973097974232
AP_mean:0.5486812148244927
F1_mean:0.6114157816653698
computer mAP...
computer PR...
epoch:0290
Precision:[0.3214814814814815, 0.8465527594300999, 0.6634935497295048, 0.325, 0.9365448311929804]
Recall:[0.23459459459459459, 0.9136864431744536, 0.7306943171402384, 0.2493150684931507, 0.9107193309600498]
AP:[0.13140100699461738, 0.8996208307068272, 0.6391271418584484, 0.1355326597841778, 0.9310273717906433]
F1:[0.2712499999999999, 0.8788393970304885, 0.695474372955289, 0.2821705426356588, 0.923451555606486]
Precision_mean:0.6186145243668133
Recall_mean:0.6078019508724974
AP_mean:0.5473418022269427
F1_mean:0.6102371736455844
