训练配置:
{'model_name': 'coco_person', 'epochs': 300, 'steps': [150.0, 250.0], 'batch_size': 128, 'subdivisions': 1, 'learning_rate': 0.001, 'pre_weights': 'default', 'classes': 1, 'width': 256, 'height': 256, 'anchor_num': 3, 'separation': 3, 'separation_scale': 2, 'anchors': [6.21, 14.8, 18.67, 43.63, 38.13, 100.64, 75.28, 171.12, 143.62, 75.3, 173.09, 209.01], 'conf_thr': 0.001, 'nms_thr': 0.5, 'iou_thr': 0.4, 'label_flag': 'coco_person', 'val': '../../../datasets/coco2017/val2017_person.txt', 'train': '../../../datasets/coco2017/train2017_person.txt', 'names': './configs/coco_person.names'}
initialize_weights...
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 24, 64, 64]             648
       BatchNorm2d-2           [-1, 24, 64, 64]              48
              ReLU-3           [-1, 24, 64, 64]               0
         MaxPool2d-4           [-1, 24, 32, 32]               0
            Conv2d-5           [-1, 24, 16, 16]             216
       BatchNorm2d-6           [-1, 24, 16, 16]              48
            Conv2d-7           [-1, 24, 16, 16]             576
       BatchNorm2d-8           [-1, 24, 16, 16]              48
              ReLU-9           [-1, 24, 16, 16]               0
           Conv2d-10           [-1, 24, 32, 32]             576
      BatchNorm2d-11           [-1, 24, 32, 32]              48
             ReLU-12           [-1, 24, 32, 32]               0
           Conv2d-13           [-1, 24, 16, 16]             216
      BatchNorm2d-14           [-1, 24, 16, 16]              48
           Conv2d-15           [-1, 24, 16, 16]             576
      BatchNorm2d-16           [-1, 24, 16, 16]              48
             ReLU-17           [-1, 24, 16, 16]               0
  ShuffleV2Block_-18           [-1, 48, 16, 16]               0
           Conv2d-19           [-1, 24, 16, 16]             576
      BatchNorm2d-20           [-1, 24, 16, 16]              48
             ReLU-21           [-1, 24, 16, 16]               0
           Conv2d-22           [-1, 24, 16, 16]             216
      BatchNorm2d-23           [-1, 24, 16, 16]              48
           Conv2d-24           [-1, 24, 16, 16]             576
      BatchNorm2d-25           [-1, 24, 16, 16]              48
             ReLU-26           [-1, 24, 16, 16]               0
  ShuffleV2Block_-27           [-1, 48, 16, 16]               0
           Conv2d-28           [-1, 24, 16, 16]             576
      BatchNorm2d-29           [-1, 24, 16, 16]              48
             ReLU-30           [-1, 24, 16, 16]               0
           Conv2d-31           [-1, 24, 16, 16]             216
      BatchNorm2d-32           [-1, 24, 16, 16]              48
           Conv2d-33           [-1, 24, 16, 16]             576
      BatchNorm2d-34           [-1, 24, 16, 16]              48
             ReLU-35           [-1, 24, 16, 16]               0
  ShuffleV2Block_-36           [-1, 48, 16, 16]               0
           Conv2d-37           [-1, 24, 16, 16]             576
      BatchNorm2d-38           [-1, 24, 16, 16]              48
             ReLU-39           [-1, 24, 16, 16]               0
           Conv2d-40           [-1, 24, 16, 16]             216
      BatchNorm2d-41           [-1, 24, 16, 16]              48
           Conv2d-42           [-1, 24, 16, 16]             576
      BatchNorm2d-43           [-1, 24, 16, 16]              48
             ReLU-44           [-1, 24, 16, 16]               0
  ShuffleV2Block_-45           [-1, 48, 16, 16]               0
           Conv2d-46           [-1, 48, 16, 16]             432
      BatchNorm2d-47           [-1, 48, 16, 16]              96
           Conv2d-48           [-1, 48, 16, 16]           2,304
      BatchNorm2d-49           [-1, 48, 16, 16]              96
             ReLU-50           [-1, 48, 16, 16]               0
           Conv2d-51           [-1, 48, 32, 32]           2,304
      BatchNorm2d-52           [-1, 48, 32, 32]              96
             ReLU-53           [-1, 48, 32, 32]               0
           Conv2d-54           [-1, 48, 16, 16]             432
      BatchNorm2d-55           [-1, 48, 16, 16]              96
           Conv2d-56           [-1, 48, 16, 16]           2,304
      BatchNorm2d-57           [-1, 48, 16, 16]              96
             ReLU-58           [-1, 48, 16, 16]               0
  ShuffleV2Block_-59           [-1, 96, 16, 16]               0
           Conv2d-60           [-1, 48, 16, 16]           2,304
      BatchNorm2d-61           [-1, 48, 16, 16]              96
             ReLU-62           [-1, 48, 16, 16]               0
           Conv2d-63           [-1, 48, 16, 16]             432
      BatchNorm2d-64           [-1, 48, 16, 16]              96
           Conv2d-65           [-1, 48, 16, 16]           2,304
      BatchNorm2d-66           [-1, 48, 16, 16]              96
             ReLU-67           [-1, 48, 16, 16]               0
  ShuffleV2Block_-68           [-1, 96, 16, 16]               0
           Conv2d-69           [-1, 48, 16, 16]           2,304
      BatchNorm2d-70           [-1, 48, 16, 16]              96
             ReLU-71           [-1, 48, 16, 16]               0
           Conv2d-72           [-1, 48, 16, 16]             432
      BatchNorm2d-73           [-1, 48, 16, 16]              96
           Conv2d-74           [-1, 48, 16, 16]           2,304
      BatchNorm2d-75           [-1, 48, 16, 16]              96
             ReLU-76           [-1, 48, 16, 16]               0
  ShuffleV2Block_-77           [-1, 96, 16, 16]               0
           Conv2d-78           [-1, 48, 16, 16]           2,304
      BatchNorm2d-79           [-1, 48, 16, 16]              96
             ReLU-80           [-1, 48, 16, 16]               0
           Conv2d-81           [-1, 48, 16, 16]             432
      BatchNorm2d-82           [-1, 48, 16, 16]              96
           Conv2d-83           [-1, 48, 16, 16]           2,304
      BatchNorm2d-84           [-1, 48, 16, 16]              96
             ReLU-85           [-1, 48, 16, 16]               0
  ShuffleV2Block_-86           [-1, 96, 16, 16]               0
           Conv2d-87           [-1, 48, 16, 16]           2,304
      BatchNorm2d-88           [-1, 48, 16, 16]              96
             ReLU-89           [-1, 48, 16, 16]               0
           Conv2d-90           [-1, 48, 16, 16]             432
      BatchNorm2d-91           [-1, 48, 16, 16]              96
           Conv2d-92           [-1, 48, 16, 16]           2,304
      BatchNorm2d-93           [-1, 48, 16, 16]              96
             ReLU-94           [-1, 48, 16, 16]               0
  ShuffleV2Block_-95           [-1, 96, 16, 16]               0
           Conv2d-96           [-1, 48, 16, 16]           2,304
      BatchNorm2d-97           [-1, 48, 16, 16]              96
             ReLU-98           [-1, 48, 16, 16]               0
           Conv2d-99           [-1, 48, 16, 16]             432
     BatchNorm2d-100           [-1, 48, 16, 16]              96
          Conv2d-101           [-1, 48, 16, 16]           2,304
     BatchNorm2d-102           [-1, 48, 16, 16]              96
            ReLU-103           [-1, 48, 16, 16]               0
 ShuffleV2Block_-104           [-1, 96, 16, 16]               0
          Conv2d-105           [-1, 48, 16, 16]           2,304
     BatchNorm2d-106           [-1, 48, 16, 16]              96
            ReLU-107           [-1, 48, 16, 16]               0
          Conv2d-108           [-1, 48, 16, 16]             432
     BatchNorm2d-109           [-1, 48, 16, 16]              96
          Conv2d-110           [-1, 48, 16, 16]           2,304
     BatchNorm2d-111           [-1, 48, 16, 16]              96
            ReLU-112           [-1, 48, 16, 16]               0
 ShuffleV2Block_-113           [-1, 96, 16, 16]               0
          Conv2d-114           [-1, 48, 16, 16]           2,304
     BatchNorm2d-115           [-1, 48, 16, 16]              96
            ReLU-116           [-1, 48, 16, 16]               0
          Conv2d-117           [-1, 48, 16, 16]             432
     BatchNorm2d-118           [-1, 48, 16, 16]              96
          Conv2d-119           [-1, 48, 16, 16]           2,304
     BatchNorm2d-120           [-1, 48, 16, 16]              96
            ReLU-121           [-1, 48, 16, 16]               0
 ShuffleV2Block_-122           [-1, 96, 16, 16]               0
          Conv2d-123             [-1, 96, 8, 8]             864
     BatchNorm2d-124             [-1, 96, 8, 8]             192
          Conv2d-125             [-1, 96, 8, 8]           9,216
     BatchNorm2d-126             [-1, 96, 8, 8]             192
            ReLU-127             [-1, 96, 8, 8]               0
          Conv2d-128           [-1, 96, 16, 16]           9,216
     BatchNorm2d-129           [-1, 96, 16, 16]             192
            ReLU-130           [-1, 96, 16, 16]               0
          Conv2d-131             [-1, 96, 8, 8]             864
     BatchNorm2d-132             [-1, 96, 8, 8]             192
          Conv2d-133             [-1, 96, 8, 8]           9,216
     BatchNorm2d-134             [-1, 96, 8, 8]             192
            ReLU-135             [-1, 96, 8, 8]               0
 ShuffleV2Block_-136            [-1, 192, 8, 8]               0
          Conv2d-137             [-1, 96, 8, 8]           9,216
     BatchNorm2d-138             [-1, 96, 8, 8]             192
            ReLU-139             [-1, 96, 8, 8]               0
          Conv2d-140             [-1, 96, 8, 8]             864
     BatchNorm2d-141             [-1, 96, 8, 8]             192
          Conv2d-142             [-1, 96, 8, 8]           9,216
     BatchNorm2d-143             [-1, 96, 8, 8]             192
            ReLU-144             [-1, 96, 8, 8]               0
 ShuffleV2Block_-145            [-1, 192, 8, 8]               0
          Conv2d-146             [-1, 96, 8, 8]           9,216
     BatchNorm2d-147             [-1, 96, 8, 8]             192
            ReLU-148             [-1, 96, 8, 8]               0
          Conv2d-149             [-1, 96, 8, 8]             864
     BatchNorm2d-150             [-1, 96, 8, 8]             192
          Conv2d-151             [-1, 96, 8, 8]           9,216
     BatchNorm2d-152             [-1, 96, 8, 8]             192
            ReLU-153             [-1, 96, 8, 8]               0
 ShuffleV2Block_-154            [-1, 192, 8, 8]               0
          Conv2d-155             [-1, 96, 8, 8]           9,216
     BatchNorm2d-156             [-1, 96, 8, 8]             192
            ReLU-157             [-1, 96, 8, 8]               0
          Conv2d-158             [-1, 96, 8, 8]             864
     BatchNorm2d-159             [-1, 96, 8, 8]             192
          Conv2d-160             [-1, 96, 8, 8]           9,216
     BatchNorm2d-161             [-1, 96, 8, 8]             192
            ReLU-162             [-1, 96, 8, 8]               0
 ShuffleV2Block_-163            [-1, 192, 8, 8]               0
  ShuffleNetV2Sp-164  [[-1, 96, 16, 16], [-1, 192, 8, 8]]               0
          Conv2d-165             [-1, 72, 8, 8]          13,824
     BatchNorm2d-166             [-1, 72, 8, 8]             144
            ReLU-167             [-1, 72, 8, 8]               0
          Conv2d-168             [-1, 72, 8, 8]           1,800
     BatchNorm2d-169             [-1, 72, 8, 8]             144
            ReLU-170             [-1, 72, 8, 8]               0
          Conv2d-171             [-1, 72, 8, 8]           5,184
     BatchNorm2d-172             [-1, 72, 8, 8]             144
          Conv2d-173             [-1, 72, 8, 8]           1,800
     BatchNorm2d-174             [-1, 72, 8, 8]             144
            ReLU-175             [-1, 72, 8, 8]               0
          Conv2d-176             [-1, 72, 8, 8]           5,184
     BatchNorm2d-177             [-1, 72, 8, 8]             144
     DWConvblock-178             [-1, 72, 8, 8]               0
          Conv2d-179             [-1, 72, 8, 8]           1,800
     BatchNorm2d-180             [-1, 72, 8, 8]             144
            ReLU-181             [-1, 72, 8, 8]               0
          Conv2d-182             [-1, 72, 8, 8]           5,184
     BatchNorm2d-183             [-1, 72, 8, 8]             144
          Conv2d-184             [-1, 72, 8, 8]           1,800
     BatchNorm2d-185             [-1, 72, 8, 8]             144
            ReLU-186             [-1, 72, 8, 8]               0
          Conv2d-187             [-1, 72, 8, 8]           5,184
     BatchNorm2d-188             [-1, 72, 8, 8]             144
     DWConvblock-189             [-1, 72, 8, 8]               0
          Conv2d-190           [-1, 72, 16, 16]          20,736
     BatchNorm2d-191           [-1, 72, 16, 16]             144
            ReLU-192           [-1, 72, 16, 16]               0
          Conv2d-193           [-1, 72, 16, 16]           1,800
     BatchNorm2d-194           [-1, 72, 16, 16]             144
            ReLU-195           [-1, 72, 16, 16]               0
          Conv2d-196           [-1, 72, 16, 16]           5,184
     BatchNorm2d-197           [-1, 72, 16, 16]             144
          Conv2d-198           [-1, 72, 16, 16]           1,800
     BatchNorm2d-199           [-1, 72, 16, 16]             144
            ReLU-200           [-1, 72, 16, 16]               0
          Conv2d-201           [-1, 72, 16, 16]           5,184
     BatchNorm2d-202           [-1, 72, 16, 16]             144
     DWConvblock-203           [-1, 72, 16, 16]               0
          Conv2d-204           [-1, 72, 16, 16]           1,800
     BatchNorm2d-205           [-1, 72, 16, 16]             144
            ReLU-206           [-1, 72, 16, 16]               0
          Conv2d-207           [-1, 72, 16, 16]           5,184
     BatchNorm2d-208           [-1, 72, 16, 16]             144
          Conv2d-209           [-1, 72, 16, 16]           1,800
     BatchNorm2d-210           [-1, 72, 16, 16]             144
            ReLU-211           [-1, 72, 16, 16]               0
          Conv2d-212           [-1, 72, 16, 16]           5,184
     BatchNorm2d-213           [-1, 72, 16, 16]             144
     DWConvblock-214           [-1, 72, 16, 16]               0
        LightFPN-215  [[-1, 72, 16, 16], [-1, 72, 16, 16], [-1, 72, 16, 16], [-1, 72, 8, 8], [-1, 72, 8, 8], [-1, 72, 8, 8]]               0
          Conv2d-216           [-1, 12, 16, 16]             876
          Conv2d-217            [-1, 3, 16, 16]             219
          Conv2d-218            [-1, 1, 16, 16]              73
          Conv2d-219             [-1, 12, 8, 8]             876
          Conv2d-220              [-1, 3, 8, 8]             219
          Conv2d-221              [-1, 1, 8, 8]              73
================================================================
Total params: 238,496
Trainable params: 238,496
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.75
Forward/backward pass size (MB): 2283.25
Params size (MB): 0.91
Estimated Total Size (MB): 2284.91
----------------------------------------------------------------
Initialize weights: model/backbone/backbone.pth
Starting training for 300 epochs...
computer mAP...
computer PR...
epoch:0010
Precision:[0.5151737897721307]
Recall:[0.44583787713558704]
AP:[0.44639834730890343]
F1:[0.47800457933453494]
Precision_mean:0.5151737897721307
Recall_mean:0.44583787713558704
AP_mean:0.44639834730890343
F1_mean:0.47800457933453494
computer mAP...
computer PR...
epoch:0020
Precision:[0.46268525826536877]
Recall:[0.47946201381315884]
AP:[0.45260585439323686]
F1:[0.4709242647386977]
Precision_mean:0.46268525826536877
Recall_mean:0.47946201381315884
AP_mean:0.45260585439323686
F1_mean:0.4709242647386977
computer mAP...
computer PR...
epoch:0030
Precision:[0.4694906261054121]
Recall:[0.48246092330061796]
AP:[0.46281677699025436]
F1:[0.4758874148440301]
Precision_mean:0.4694906261054121
Recall_mean:0.48246092330061796
AP_mean:0.46281677699025436
F1_mean:0.4758874148440301
computer mAP...
computer PR...
epoch:0040
Precision:[0.4676737160120846]
Recall:[0.49236641221374045]
AP:[0.466408017635918]
F1:[0.4797025100712735]
Precision_mean:0.4676737160120846
Recall_mean:0.49236641221374045
AP_mean:0.466408017635918
F1_mean:0.4797025100712735
computer mAP...
computer PR...
epoch:0050
Precision:[0.527235142118863]
Recall:[0.46355870592511816]
AP:[0.4713614369227418]
F1:[0.49335074229895054]
Precision_mean:0.527235142118863
Recall_mean:0.46355870592511816
AP_mean:0.4713614369227418
F1_mean:0.49335074229895054
computer mAP...
computer PR...
epoch:0060
Precision:[0.5283727540351233]
Recall:[0.47300981461286806]
AP:[0.47466807422569934]
F1:[0.4991608726923999]
Precision_mean:0.5283727540351233
Recall_mean:0.47300981461286806
AP_mean:0.47466807422569934
F1_mean:0.4991608726923999
computer mAP...
computer PR...
epoch:0070
Precision:[0.5410944432706528]
Recall:[0.465467102871683]
AP:[0.4776906947377264]
F1:[0.5004396678065461]
Precision_mean:0.5410944432706528
Recall_mean:0.465467102871683
AP_mean:0.4776906947377264
F1_mean:0.5004396678065461
computer mAP...
computer PR...
epoch:0080
Precision:[0.4530048772422915]
Recall:[0.4980007270083606]
AP:[0.4729819085553931]
F1:[0.4744383360027704]
Precision_mean:0.4530048772422915
Recall_mean:0.4980007270083606
AP_mean:0.4729819085553931
F1_mean:0.4744383360027704
computer mAP...
computer PR...
epoch:0090
Precision:[0.4992437133673662]
Recall:[0.4799163940385314]
AP:[0.47562876330198844]
F1:[0.4893893059030673]
Precision_mean:0.4992437133673662
Recall_mean:0.4799163940385314
AP_mean:0.47562876330198844
F1_mean:0.4893893059030673
computer mAP...
computer PR...
epoch:0100
Precision:[0.5334363730036064]
Recall:[0.4704652853507815]
AP:[0.4809009590901099]
F1:[0.4999758559080593]
Precision_mean:0.5334363730036064
Recall_mean:0.4704652853507815
AP_mean:0.4809009590901099
F1_mean:0.4999758559080593
computer mAP...
computer PR...
epoch:0110
Precision:[0.43984569637547216]
Recall:[0.49736459469283895]
AP:[0.46698547166721877]
F1:[0.4668401074764362]
Precision_mean:0.43984569637547216
Recall_mean:0.49736459469283895
AP_mean:0.46698547166721877
F1_mean:0.4668401074764362
computer mAP...
computer PR...
epoch:0120
Precision:[0.49788275148207395]
Recall:[0.48082515448927665]
AP:[0.47348486897955294]
F1:[0.4892053071980028]
Precision_mean:0.49788275148207395
Recall_mean:0.48082515448927665
AP_mean:0.47348486897955294
F1_mean:0.4892053071980028
computer mAP...
computer PR...
epoch:0130
Precision:[0.48654358409111786]
Recall:[0.48137041075972375]
AP:[0.47401551635787476]
F1:[0.4839431729934676]
Precision_mean:0.48654358409111786
Recall_mean:0.48137041075972375
AP_mean:0.47401551635787476
F1_mean:0.4839431729934676
computer mAP...
computer PR...
epoch:0140
Precision:[0.5070861678004536]
Recall:[0.48773173391494]
AP:[0.48560124062021576]
F1:[0.49722067815453025]
Precision_mean:0.5070861678004536
Recall_mean:0.48773173391494
AP_mean:0.48560124062021576
F1_mean:0.49722067815453025
computer mAP...
computer PR...
epoch:0150
Precision:[0.5087882451408909]
Recall:[0.4971828426026899]
AP:[0.4900487645899261]
F1:[0.502918600910052]
Precision_mean:0.5087882451408909
Recall_mean:0.4971828426026899
AP_mean:0.4900487645899261
F1_mean:0.502918600910052
computer mAP...
computer PR...
epoch:0160
Precision:[0.4942012047109593]
Recall:[0.4995456197746274]
AP:[0.4889207813994713]
F1:[0.4968590409906449]
Precision_mean:0.4942012047109593
Recall_mean:0.4995456197746274
AP_mean:0.4889207813994713
F1_mean:0.4968590409906449
computer mAP...
computer PR...
epoch:0170
Precision:[0.4837969094922737]
Recall:[0.4979098509632861]
AP:[0.48491892582392915]
F1:[0.4907519369429889]
Precision_mean:0.4837969094922737
Recall_mean:0.4979098509632861
AP_mean:0.48491892582392915
F1_mean:0.4907519369429889
computer mAP...
computer PR...
epoch:0180
Precision:[0.47691776684787296]
Recall:[0.5022719011268629]
AP:[0.485413894785157]
F1:[0.48926658699597214]
Precision_mean:0.47691776684787296
Recall_mean:0.5022719011268629
AP_mean:0.485413894785157
F1_mean:0.48926658699597214
computer mAP...
computer PR...
epoch:0190
Precision:[0.4882473858253642]
Recall:[0.4964558342420938]
AP:[0.48352337285464064]
F1:[0.4923173973775514]
Precision_mean:0.4882473858253642
Recall_mean:0.4964558342420938
AP_mean:0.48352337285464064
F1_mean:0.4923173973775514
computer mAP...
computer PR...
epoch:0200
Precision:[0.48680926916221035]
Recall:[0.49636495819701926]
AP:[0.4837874670839385]
F1:[0.49154067674586027]
Precision_mean:0.48680926916221035
Recall_mean:0.49636495819701926
AP_mean:0.4837874670839385
F1_mean:0.49154067674586027
computer mAP...
computer PR...
epoch:0210
Precision:[0.48205489092188597]
Recall:[0.4980007270083606]
AP:[0.481071979530311]
F1:[0.4898980868943321]
Precision_mean:0.48205489092188597
Recall_mean:0.4980007270083606
AP_mean:0.481071979530311
F1_mean:0.4898980868943321
computer mAP...
computer PR...
epoch:0220
Precision:[0.48707824838478103]
Recall:[0.4932751726644856]
AP:[0.47795013262348607]
F1:[0.49015712479682133]
Precision_mean:0.48707824838478103
Recall_mean:0.4932751726644856
AP_mean:0.47795013262348607
F1_mean:0.49015712479682133
computer mAP...
computer PR...
epoch:0230
Precision:[0.4767178235088943]
Recall:[0.49681933842239184]
AP:[0.4802938570767527]
F1:[0.4865610537557849]
Precision_mean:0.4767178235088943
Recall_mean:0.49681933842239184
AP_mean:0.4802938570767527
F1_mean:0.4865610537557849
computer mAP...
computer PR...
epoch:0240
Precision:[0.47759495886574477]
Recall:[0.49591057797164667]
AP:[0.47835567082505426]
F1:[0.48658047258136417]
Precision_mean:0.47759495886574477
Recall_mean:0.49591057797164667
AP_mean:0.47835567082505426
F1_mean:0.48658047258136417
computer mAP...
computer PR...
epoch:0250
Precision:[0.48229893175598126]
Recall:[0.4964558342420938]
AP:[0.4818651306164456]
F1:[0.48927499888048]
Precision_mean:0.48229893175598126
Recall_mean:0.4964558342420938
AP_mean:0.4818651306164456
F1_mean:0.48927499888048
computer mAP...
computer PR...
epoch:0260
Precision:[0.4751477233229058]
Recall:[0.49691021446746636]
AP:[0.4802259352360154]
F1:[0.48578535891968716]
Precision_mean:0.4751477233229058
Recall_mean:0.49691021446746636
AP_mean:0.4802259352360154
F1_mean:0.48578535891968716
computer mAP...
computer PR...
epoch:0270
Precision:[0.4729823346033945]
Recall:[0.49636495819701926]
AP:[0.4797141444963346]
F1:[0.4843916282369634]
Precision_mean:0.4729823346033945
Recall_mean:0.49636495819701926
AP_mean:0.4797141444963346
F1_mean:0.4843916282369634
computer mAP...
computer PR...
epoch:0280
Precision:[0.4695957519698527]
Recall:[0.4982733551435842]
AP:[0.4797961534816815]
F1:[0.48350970017636685]
Precision_mean:0.4695957519698527
Recall_mean:0.4982733551435842
AP_mean:0.4797961534816815
F1_mean:0.48350970017636685
computer mAP...
computer PR...
epoch:0290
Precision:[0.47186481386172585]
Recall:[0.4999091239549255]
AP:[0.48086352953991973]
F1:[0.4854823051804783]
Precision_mean:0.47186481386172585
Recall_mean:0.4999091239549255
AP_mean:0.48086352953991973
F1_mean:0.4854823051804783
